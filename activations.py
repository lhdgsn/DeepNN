# activations.py
# non-linear activation functions
# Liam Hodgson
# 30 November 2017

import numpy as np

def relu(x):
	return np.maximum(x, 0)

def sigmoid(x):

def softmax(x):